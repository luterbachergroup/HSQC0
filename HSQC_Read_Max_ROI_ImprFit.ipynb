{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HSQC fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import seaborn as sns\n",
    "from scipy.optimize import curve_fit\n",
    "plt.ion()\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import math\n",
    "from matplotlib import rc, rcParams\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_lig = 51.7 #mass of material (extracted lignin/ cell wall) in mg\n",
    "m_PS = 7.1 #mass of polystyrene standard in mg\n",
    "MW_lig = #molecular weight of lignin repetiting unit \n",
    "MW_PS = 102  #molecular weight of polystyrene standard repetiting unit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Functions generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of local maxima based on neighbouring intensity. The output is sorted in descending intensity order.\n",
    "def detect_local_maxima(data, x_names, y_names, threshold = 0):\n",
    "    local_max = []\n",
    "    for j in range(1, data.shape[1] - 1):\n",
    "        for i in range(1, data.shape[0] - 1):\n",
    "            middle = data[i][j]\n",
    "            top = data[i][j - 1]\n",
    "            bottom = data[i][j + 1]\n",
    "            left = data[i - 1][j]\n",
    "            right = data[i + 1][j]\n",
    "            if (middle > top and\n",
    "                middle > bottom and\n",
    "                middle > left and\n",
    "                middle > right and\n",
    "                middle > threshold):\n",
    "                # (intensity, x_index, y_index)\n",
    "                local_max += [(middle, i, j, x_names[j], y_names[i] )]\n",
    "    local_max = sorted(local_max)\n",
    "    local_max.reverse()\n",
    "    return local_max\n",
    "\n",
    "# 2D Gaussian function with independent components.\n",
    "def gauss2d(Z, A, mu_x, mu_y, sigma_x, sigma_y):\n",
    "    x = np.array(Z[0])\n",
    "    y = np.array(Z[1])\n",
    "    x_exp = np.exp(-0.5 * ((x - mu_x)/sigma_x) ** 2)\n",
    "    y_exp = np.exp(-0.5 * ((y - mu_y)/sigma_y) ** 2)\n",
    "    ret = (A * x_exp * y_exp)\n",
    "    return ret.ravel()\n",
    "\n",
    "def calculate_r2(X, Y, Z, popt):\n",
    "    Z_fit = gauss2d((X, Y), popt[0], popt[1], popt[2], popt[3], popt[4])\n",
    "    Z = Z.ravel()\n",
    "    residuals = Z - Z_fit\n",
    "    print (residuals)\n",
    "    ss_res = np.sum(residuals ** 2)\n",
    "    ss_tot = np.sum((Z - np.mean(Z)) ** 2)\n",
    "    print(ss_tot, ss_res)\n",
    "    R2 = 1 -(ss_res / ss_tot)\n",
    "    return R2\n",
    "\n",
    "def calculate_r2_2(X, Y, Z, params):\n",
    "    Z_fit = gauss2d_2((X, Y), *popt)\n",
    "    Z = Z.ravel()\n",
    "    residuals = Z - Z_fit\n",
    "    print('residuals', residuals)\n",
    "    ss_res = np.sum(residuals ** 2)\n",
    "    ss_tot = np.sum((Z - np.mean(Z)) ** 2)\n",
    "    print(ss_tot, ss_res)\n",
    "    R2 = 1 -(ss_res / ss_tot)\n",
    "    return R2\n",
    "\n",
    "def gauss_1d_x (x, A, x_0, sigma_x):\n",
    "  C =  A*(1/(sigma_x*(np.sqrt(2*np.pi))))*(np.exp((-1.0/2.0)*(((x-x_0)/sigma_x)**2)))\n",
    "  return C #return the gaussian fitting \n",
    "\n",
    "\n",
    "# 2D Gaussian function with overlapping.\n",
    "def gauss2d_2(Z, A, mu_x, mu_y, sigma_x, sigma_y, B, mu_x_2, mu_y_2, sigma_x_2, sigma_y_2):\n",
    "    x = np.array(Z[0])\n",
    "    y = np.array(Z[1])\n",
    "    x_exp = np.exp(-0.5 * ((x - mu_x)/sigma_x) ** 2) \n",
    "    x_exp_2 = np.exp(-0.5 * ((x - mu_x_2)/sigma_x_2) ** 2)\n",
    "    y_exp = np.exp(-0.5 * ((y - mu_y)/sigma_y) ** 2)\n",
    "    y_exp_2 = np.exp(-0.5 * ((y - mu_y_2)/sigma_y_2) ** 2)\n",
    "    ret = (A * x_exp * y_exp + B * x_exp_2 * y_exp_2)\n",
    "    return ret.ravel()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the three HSQC text file previously extracted witg rbnmr or NMRpipe\n",
    "data_1 = pd.read_csv('/Users/xxx/Documents/nmr/Birch_1/1/pdata/1/Data.txt', sep='\\t', header=None) #HSQC_1\n",
    "data_2 = pd.read_csv('/Users/xxx/Documents/nmr/Birch_1/2/pdata/1/Data.txt', sep='\\t', header=None) #HSQC_2\n",
    "data_3 = pd.read_csv('/Users/xxx/Documents/nmr/Birch_1/2/pdata/1/Data.txt', sep='\\t', header=None) #HSQC_3\n",
    "x = pd.read_csv('/Users/xxx/Documents/nmr/Birch_1/2/pdata/1/X.txt', sep = \"\\t\",  header=None) #x axis - 1H (ppm)\n",
    "y = pd.read_csv('/Users/xxx/Documents/nmr/Birch_1/2/pdata/1/Y.txt', sep = \"\\t\", header=None) #y axis - 1H (ppm)\n",
    "\n",
    "Data_1 = data_1.values \n",
    "Data_2 = data_2.values\n",
    "Data_3 = data_3.values\n",
    "data = [] #creating one big data set with three spectra\n",
    "data.append(Data_1)\n",
    "data.append(Data_2)\n",
    "data.append(Data_3)\n",
    "\n",
    "X = x.values\n",
    "Y = y.values\n",
    "x_names = X.flatten()\n",
    "y_names = Y.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot a 2D image of the matrix\n",
    "hfont = {'fontname':'Arial'}\n",
    "ax = plt.pcolormesh(x_names, y_names, data_1, vmin=-10., vmax=10., shading='auto', cmap='hsv')\n",
    "ax = plt.axes()\n",
    "ax.invert_xaxis()\n",
    "ax.invert_yaxis()\n",
    "ax.yaxis.tick_right()\n",
    "ax.text('1H (ppm)', '13C (ppm)', 'Intensity')\n",
    "plt.xlabel('1H (ppm)', **hfont)\n",
    "plt.ylabel('13C (ppm)', **hfont)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract peaks of interest out of the NMR spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "HSQC = np.array([1,2,3])\n",
    "\n",
    "for h in range (0, (len(data)), 1): \n",
    "  data_df = pd.DataFrame(data[h], columns=x_names, index=y_names)  #Open HSQC spectra successively \n",
    "  print(\"Detecting local maxima\")\n",
    "  local_max = detect_local_maxima(data[h], x_names, y_names, 1e3)\n",
    "  print(\"> found: \", len(local_max))\n",
    "  #print (local_max)\n",
    "  \n",
    "  #Create as many lists of indexes as peaks you want to analyse \n",
    "  ind = [] \n",
    "  ind_PS = [] #standard peak\n",
    "  ind_Alpha = [] #B-O-4 alpha peak\n",
    "  ind_Beta = [] #B-O-4 beta peak\n",
    "    \n",
    "  for k in range (0, len(local_max)): #look for our peaks of interest: alpha, beta, gamma, acetal, IS \n",
    "  #specify in which region (x_up, x_down, y_up, y_down) you can find your peak of interest.\n",
    "  #do it for all the peaks defined above\n",
    "    if (local_max[k][4] >= y_down and local_max[k][4] <= y_up and local_max[k][3] >= x_down and local_max[k][3] <= x_up):\n",
    "      ind_Alpha.append(k)\n",
    "    elif (local_max[k][4] >= 39 and local_max[k][4] <= 43 and local_max[k][3] >= 1.7 and local_max[k][3] <= 2.3): \n",
    "      ind_PS.append(k)\n",
    "  print('PS', ind_PS, 'Alpha', ind_Alpha, 'Beta', ind_Beta)\n",
    "\n",
    "\n",
    "  #Select the highest local maxima as the peak to be fitted for each peak of interest\n",
    "  #Do it if the region you have specified is big and several peaks have been found\n",
    "  int_Alpha = [local_max[q][0] for q in ind_Alpha]\n",
    "  max_a = np.max(int_Alpha)\n",
    "  index_a = np.where(int_Alpha == max_a)\n",
    "  index_a = index_a[0]\n",
    "  print(index_a)\n",
    "  ind.append(ind_Alpha[index_a[0]])\n",
    "\n",
    "  int_Beta = [local_max[q][0] for q in ind_Beta]\n",
    "  max_b = np.max(int_Beta)\n",
    "  index_b = np.where(int_Beta == max_b)\n",
    "  index_b = index_b[0]\n",
    "  ind.append(ind_Beta[index_b[0]])\n",
    "\n",
    "  #Always put the standard as last item of the list\n",
    "  int_PS = [local_max[q][0] for q in ind_PS]\n",
    "  max_PS = np.max(int_PS)\n",
    "  index_PS = np.where(int_Apo == max_PS)\n",
    "  index_PS = index_PS[0]\n",
    "  ind.append(ind_PS[index_PS[0]])\n",
    "\n",
    "  print(ind)\n",
    "\n",
    "  candidate_peaks=[]\n",
    "  #Adapt the following list depending on the number of peaks fitted\n",
    "  candidate_peaks = list([local_max[ind[0]], local_max[ind[1]], local_max[ind[2]]])\n",
    "  #Generate the final list of peaks we will work on\n",
    "  print(candidate_peaks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis and gaussian fitting of local maxima: extracting peaks data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the analysis of the selected N local maxima.\n",
    "# Define a window we will work on to fit the n peaks - to be adapted and reduced in case of overlaping peaks\n",
    "  roi_n_samples_x_min = [[6, 6, 6], [6, 6, 6], [6, 6, 6]]  \n",
    "  roi_n_samples_x_max = [[6, 6, 6], [6, 6, 6], [6, 6, 6]]\n",
    "  roi_n_samples_y = [6, 6, 6]\n",
    "\n",
    "    \n",
    "  #Confidence interval\n",
    "  ci = 0.95\n",
    "  # Convert to percentile point of the normal distribution.\n",
    "  # See: https://en.wikipedia.org/wiki/Standard_score\n",
    "  pp = (1. + ci) / 2.\n",
    "  # Convert to number of standard deviations.\n",
    "  nstd = stats.norm.ppf(pp)\n",
    "  print (nstd)\n",
    "  \n",
    "  plt.close('all')\n",
    "  plt.ioff()\n",
    "  fitted_params = []\n",
    "  param_names = [\n",
    "    \"local_max_x\",\n",
    "    \"local_max_y\",\n",
    "    \"local_max_intensity\",\n",
    "    \"fitted_height\",\n",
    "    \"fitted_x\",\n",
    "    \"fitted_y\",\n",
    "    \"fitted_sigma_x\",\n",
    "    \"fitted_sigma_y\",\n",
    "    \"fitted_volume\",\n",
    "    \"fit_R2\"]\n",
    "  \n",
    "  for (k, candidate) in enumerate(candidate_peaks):\n",
    "    intensity, i, j, x_h, y_c = candidate\n",
    "    x0 = data_df.columns[j]\n",
    "    y0 = data_df.index[i]\n",
    "    print(k)\n",
    "\n",
    "    # Calculate the region of interest (ROI) for this candidate peak\n",
    "    min_x = j - roi_n_samples_x_min[h][k]\n",
    "    max_x = j + roi_n_samples_x_max[h][k]\n",
    "    min_y = i - roi_n_samples_y[k]\n",
    "    max_y = i + roi_n_samples_y[k]\n",
    "    peak_data = data_df.iloc[min_y:max_y, min_x:max_x] #ROI\n",
    "    print(peak_data)\n",
    "    peak_data_red = data_df.iloc[min_y:max_y, min_x:max_x]\n",
    "\n",
    "    # Prepare the parameter matrix for curve fit.\n",
    "    x_values = peak_data.columns\n",
    "    y_values = peak_data.index\n",
    "    print(x_values)\n",
    "    X, Y = np.meshgrid(x_values, y_values)\n",
    "\n",
    "    # Initial fit parameters.\n",
    "    p0 = (intensity, x0, y0, 0.01, 0.05)\n",
    "    print(p0)\n",
    "\n",
    "    # The bounds can be used to constrain each parameter fitting to a range.\n",
    "    tol_x = 0.5\n",
    "    tol_y = 0.5\n",
    "    bounds = (\n",
    "        (1e3, x0 - tol_x, y0 - tol_y, 0.001, 0.01),\n",
    "        (1e12, x0 + tol_x, y0 + tol_y, 1, 50)\n",
    "        )\n",
    "        \n",
    "    # If fitting fails for this candidate peak, we move on to the next one.\n",
    "    try: # Do the thing!\n",
    "      popt, pcov = curve_fit(gauss2d, (X, Y), peak_data_red.values.ravel(), p0=p0, bounds=bounds)\n",
    "      print(popt)\n",
    "      # Standard deviation errors on the parameters.\n",
    "      perr = np.sqrt(np.diag(pcov))\n",
    "      # Add nstd standard deviations to parameters to obtain the upper confidence interval.\n",
    "      popt_up = popt + nstd * perr\n",
    "      popt_dw = popt - nstd * perr\n",

    "    except:\n",
    "      print('Failed')\n",
    "      continue\n",
    "    \n",
    "    # Prepare the fitted data for a contour plotting. We use 100 points for higher resolution of circles, which looks nicer.\n",
    "    x_fit = np.linspace(np.min(x_values), np.max(x_values), 100)\n",
    "    y_fit = np.linspace(np.min(y_values), np.max(y_values), 100)\n",
    "    X_fit, Y_fit = np.meshgrid(x_fit, y_fit)\n",
    "    peak_data_fit = gauss2d((X_fit, Y_fit), *popt).reshape(100, 100)\n",
    "\n",
    "     Calculate volume.\n",
    "    # If using the overlaping function: \n",
    "    # fit_height, fit_x0, fit_y0, fit_sigma_x, fit_sigma_y, fit_height_2, fit_x0_2, fit_y0_2, fit_sigma_x_2, fit_sigma_y_2) = popt\n",
    "    (fit_height, fit_x0, fit_y0, fit_sigma_x, fit_sigma_y) = (popt[0], popt[1], popt[2], popt[3], popt[4])\n",
    "    fit_volume = fit_height * fit_sigma_x * fit_sigma_y * 2.0 * np.pi \n",
    "        \n",
    "    # Calculate R2, but only within N * sigma in each direction, since multiple peaks may appear in the ROI.\n",
    "    n_sig_x = 4 #adapt the value based on your data\n",
    "    n_sig_y = 300\n",
    "    x_index = ((peak_data.columns > x0 - fit_sigma_x * n_sig_x) &\n",
    "                (peak_data.columns < x0 + fit_sigma_x * n_sig_x))\n",
    "    y_index = ((peak_data.index > y0 - fit_sigma_y * n_sig_y) &\n",
    "               (peak_data.index < y0 + fit_sigma_y * n_sig_y))\n",
    "        \n",
    "\n",
    "    x_r2 = peak_data.columns[x_index]\n",
    "    y_r2 = peak_data.index[y_index]\n",
    "    X_r2, Y_r2 = np.meshgrid(x_r2, y_r2)\n",
    "    Z_r2 = peak_data_red.iloc[y_index, x_index]\n",
    "\n",
    "   # peak_data\n",
    "   fit_r2 = calculate_r2(X_r2, Y_r2, Z_r2.values, popt) #use calculate_r2_2 if use of gauss2d_2\n",
    "   fitted_params = [(h+1), x0, y0, intensity, popt[0], popt[1], popt[2], popt[3], popt[4], fit_volume, np.log(fit_volume), fit_r2, popt_up[0], popt_up[1], popt_up[2], popt_up[3], popt_up[4], popt_down[0], popt_down[1], popt_down[2], popt_down[3], popt_down[4]] #, fit_r2)]\n",
    "   print(fitted_params)\n",
    "   results.append(fitted_params)\n",
    "\n",
    "## Data plotting: 2D / 3D with corresponding fit. \n",
    "\n",
    "   # Plot the candidate peaks in 2D and the corresponding fit.\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes()\n",
    "    ax.pcolormesh(peak_data.columns, peak_data.index, peak_data, shading='auto')\n",
    "    ax.contour(X_fit, Y_fit, peak_data_fit, 5, colors='w', alpha=0.8)\n",
    "    ax.invert_xaxis()\n",
    "    ax.invert_yaxis()\n",
    "    ax.yaxis.tick_right()\n",
    "    plt.xlabel('1H (ppm)', **hfont, fontweight='bold')\n",
    "    plt.ylabel('13C (ppm)', **hfont, fontweight='bold')\n",
    "    ax.xaxis.set_tick_params(labelsize=10)\n",
    "    ax.yaxis.set_tick_params(labelsize=10)\n",
    "    \n",
    "    \n",
    "\n",
    "    fig3d = plt.figure()\n",
    "    ax3d = plt.axes(projection='3d')\n",
    "    ax3d.contour3D(X_fit, Y_fit, peak_data_fit, 50, cmap='summer')\n",
    "    ax3d.plot_wireframe(X, Y, peak_data_red.values, color='red')\n",
    "    \n",
    "    plt.xlabel('1H (ppm)', **hfont, fontweight='bold')\n",
    "    plt.ylabel('13C (ppm)', **hfont, fontweight='bold')\n",
    "    \n",
    "    # Save plots do disk.\n",
    "    plt.title(\"HSQC: {} x: {}  y: {}  int: {} R2: {:2f}\".format((h+1), x0, y0, intensity, fit_r2))\n",
    "    fig_name = \"{}_{}_{}.png\".format(h, x0, y0)\n",
    "    plt.savefig(fig_name, dpi=300)\n",
    "    #plt.close(fig)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save fitted peak parameters to disk.\n",
    "param_names = ['h', 'x0', 'y0', 'Intensity', 'A', 'mu_x', 'mu_y', 'sigma_x', 'sigma_y', 'volume', 'ln(V)', 'r2','A', 'mu_x', 'mu_y', 'sigma_x', 'sigma_y', 'A', 'mu_x', 'mu_y', 'sigma_x', 'sigma_y','V0', 'BO4']\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing extracted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse volume results\n",
    "ln_V_0 = []\n",
    "for n in range(1,x+1,1): #Adapt this number based on the number x of peaks you fitted: you have 3 spectra and x peaks in each spectra\n",
    "    y = []\n",
    "    for m in range (0, 3*x, 1): #Adapt this number based on the number x of peaks you fitted: you have 3 spectra and x peaks in each spectra\n",
    "      if results[m][0] == n:\n",
    "        y.append(results[m][10])\n",
    "    ln_V_0.append(y)\n",
    "print(ln_V_0)\n",
    "\n",
    "V_0 = []\n",
    "for p in range (0, x, 1): ): #Adapt this number based on the number x of peaks you fitted\n",
    "  model = LinearRegression()\n",
    "  y = np.array([ln_V_0[q][p] for q in range(0,3,1)])\n",
    "  model.fit(HSQC.reshape(-1, 1), y.reshape(-1, 1))\n",
    "  zero = model.intercept_\n",
    "  V_0.append((zero))\n",
    "print(V_0)\n",
    "\n",
    "\n",
    "n_PS = m_PS / MW_PS #mole of polystyrene in the tube\n",
    "n_BO4 = []  #mole of B-O-4 linkages in the tube\n",
    "n_BO4_mass = [] #mole of B-O-4 linkages in the tube per g of lignin\n",
    "\n",
    "for k in range (0, len(V_0) -1 ,1):\n",
    "  n_BO4.append(n_PS*np.exp(V_0[k])/np.exp(V_0[k])) #k is the index corresponding to the PS peak\n",
    "  n_BO4_mass.append((n_PS*np.exp(V_0[k])/np.exp(V_0[k]))/(m_lig*0.001)) #k is the index corresponding to the PS peak\n",
    "print(n_BO4)\n",
    "print('mass', n_BO4_mass)\n",
    "\n",
    "for k in range (0, x-1, 1):\n",
    "    results[k].append(str(V_0[k]))\n",
    "    results[k].append(str(n_BO4_mass[k]))\n",
    "fitted_params_df = pd.DataFrame(results, columns=param_names)\n",
    "fitted_params_df.to_csv(\"fitted_parameters.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
